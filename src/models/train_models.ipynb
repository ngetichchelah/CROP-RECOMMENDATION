{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29dc6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, f1_score, precision_score, \n",
    "                             recall_score)\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22c6c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Yvonne\\Desktop\\DATA SCIENCE\\FINAL CAPSTONE\\crop-recommendation-system\\src\\models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "#print(\"Looking for file at:\", self.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a6cacb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CROP RECOMMENDATION SYSTEM - MODEL TRAINING\n",
      "LOADING DATA\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/Crop_recommendation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 364\u001b[39m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMODEL TRAINING COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 341\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    338\u001b[39m trainer = CropRecommendationModel()\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# Train all models\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m X_test, y_test, feature_names = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_all_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;66;03m# Compare models\u001b[39;00m\n\u001b[32m    344\u001b[39m comparison_df, best_model_name = trainer.compare_models()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mCropRecommendationModel.train_all_models\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Train all models and evaluate\"\"\"\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m X_train, X_test, y_train, y_test, feature_names = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m.train_random_forest(X_train, y_train)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mCropRecommendationModel.load_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLOADING DATA\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Separate features and target\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Desktop\\DATA SCIENCE\\FINAL CAPSTONE\\crop-recommendation-system\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Desktop\\DATA SCIENCE\\FINAL CAPSTONE\\crop-recommendation-system\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Desktop\\DATA SCIENCE\\FINAL CAPSTONE\\crop-recommendation-system\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Desktop\\DATA SCIENCE\\FINAL CAPSTONE\\crop-recommendation-system\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yvonne\\Desktop\\DATA SCIENCE\\FINAL CAPSTONE\\crop-recommendation-system\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/raw/Crop_recommendation.csv'"
     ]
    }
   ],
   "source": [
    "class CropRecommendationModel:\n",
    "    \"\"\"Class to handle crop recommendation model training and evaluation\"\"\"\n",
    "    \n",
    "    # def __init__(self):\n",
    "    #     # go up two levels from 'src/models' to reach project root\n",
    "    #     base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    #     self.data_path = os.path.join(base_dir, 'data', 'processed', 'crop_data_with_features.csv')\n",
    "\n",
    "    #def __init__(self, data_path='data/processed/crop_data_cleaned.csv'):\n",
    "    #def __init__(self, data_path='data/processed/crop_data_with_features.csv'):\n",
    "    def __init__(self, data_path='data/raw/Crop_recommendation.csv'):\n",
    "        \"\"\"Initialize with data path\"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = None\n",
    "        self.label_encoder = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and prepare data\"\"\"\n",
    "        print(\"LOADING DATA\")\n",
    "        \n",
    "        # Load dataset\n",
    "        df = pd.read_csv(self.data_path)\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df.drop('label', axis=1)\n",
    "        y = df['label']\n",
    "        \n",
    "        print(f\"Features: {list(X.columns)}\")\n",
    "        print(f\"Number of classes: {y.nunique()}\")\n",
    "        print(f\"Classes: {sorted(y.unique())}\")\n",
    "        \n",
    "        # Encode target labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Split data (80-20)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "        print(f\"Testing samples: {len(X_test)}\")\n",
    "                \n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, X.columns\n",
    "    \n",
    "    def train_random_forest(self, X_train, y_train):\n",
    "        \"\"\"Train Random Forest model\"\"\"        \n",
    "        print(\"TRAINING RANDOM FOREST\")\n",
    "                \n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=20,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf_model.fit(X_train, y_train)\n",
    "        self.models['Random Forest'] = rf_model\n",
    "        \n",
    "        print(\"Random Forest trained successfully\")\n",
    "        return rf_model\n",
    "    \n",
    "    def train_xgboost(self, X_train, y_train):\n",
    "        \"\"\"Train XGBoost model\"\"\"        \n",
    "        print(\"TRAINING XGBOOST\")\n",
    "                \n",
    "        xgb_model = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        \n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        self.models['XGBoost'] = xgb_model\n",
    "        \n",
    "        print(\"XGBoost trained successfully\")\n",
    "        return xgb_model\n",
    "    \n",
    "    def train_svm(self, X_train, y_train):\n",
    "        \"\"\"Train SVM model\"\"\"        \n",
    "        print(\"TRAINING SVM\")\n",
    "                \n",
    "        svm_model = SVC(\n",
    "            kernel='rbf',\n",
    "            C=10,\n",
    "            gamma='scale',\n",
    "            random_state=42,\n",
    "            probability=True\n",
    "        )\n",
    "        \n",
    "        svm_model.fit(X_train, y_train)\n",
    "        self.models['SVM'] = svm_model\n",
    "        \n",
    "        print(\"SVM trained successfully\")\n",
    "        return svm_model\n",
    "    \n",
    "    def train_knn(self, X_train, y_train):\n",
    "        \"\"\"Train KNN model\"\"\"        \n",
    "        print(\"TRAINING KNN\")\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn_model.fit(X_train, y_train)\n",
    "        self.models['KNN'] = knn_model\n",
    "        \n",
    "        print(\"KNN trained successfully\")\n",
    "        return knn_model\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test, model_name):\n",
    "        \"\"\"Evaluate a single model\"\"\"\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1-Score:  {f1:.4f}\")\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def train_all_models(self):\n",
    "        \"\"\"Train all models and evaluate\"\"\"\n",
    "        # Load data\n",
    "        X_train, X_test, y_train, y_test, feature_names = self.load_data()\n",
    "        \n",
    "        # Train models\n",
    "        self.train_random_forest(X_train, y_train)\n",
    "        self.train_xgboost(X_train, y_train)\n",
    "        self.train_svm(X_train, y_train)\n",
    "        self.train_knn(X_train, y_train)\n",
    "        \n",
    "        # Evaluate all models\n",
    "        print(\"MODEL EVALUATION\")\n",
    "                \n",
    "        for model_name, model in self.models.items():\n",
    "            self.evaluate_model(model, X_test, y_test, model_name)\n",
    "        \n",
    "        # Return test data for visualization\n",
    "        return X_test, y_test, feature_names\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare all models\"\"\"\n",
    "        print(\"MODEL COMPARISON\")\n",
    "                \n",
    "        comparison_df = pd.DataFrame(self.results).T\n",
    "        comparison_df = comparison_df[['accuracy', 'precision', 'recall', 'f1_score']]\n",
    "        comparison_df = comparison_df.round(4)\n",
    "        \n",
    "        print(comparison_df)\n",
    "        \n",
    "        # Find best model\n",
    "        best_model_name = comparison_df['accuracy'].idxmax()\n",
    "        best_accuracy = comparison_df['accuracy'].max()\n",
    "        \n",
    "        print(f\"\\nBest Model: {best_model_name}\")\n",
    "        print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
    "        \n",
    "        return comparison_df, best_model_name\n",
    "    \n",
    "    def plot_model_comparison(self, comparison_df, save_path='results/figures/model_comparison.png'):\n",
    "        \"\"\"Plot model comparison\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Bar plot for accuracy\n",
    "        axes[0].barh(comparison_df.index, comparison_df['accuracy'], color='steelblue')\n",
    "        axes[0].set_xlabel('Accuracy', fontsize=12)\n",
    "        axes[0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlim(0.90, 1.0)\n",
    "        for i, v in enumerate(comparison_df['accuracy']):\n",
    "            axes[0].text(v + 0.002, i, f'{v:.4f}', va='center')\n",
    "        \n",
    "        # Grouped bar plot for all metrics\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "        x = np.arange(len(comparison_df.index))\n",
    "        width = 0.2\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            offset = (i - 1.5) * width\n",
    "            axes[1].bar(x + offset, comparison_df[metric], width, \n",
    "                       label=metric.replace('_', ' ').title())\n",
    "        \n",
    "        axes[1].set_xlabel('Models', fontsize=12)\n",
    "        axes[1].set_ylabel('Score', fontsize=12)\n",
    "        axes[1].set_title('All Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(comparison_df.index, rotation=45, ha='right')\n",
    "        axes[1].legend()\n",
    "        axes[1].set_ylim(0.90, 1.0)\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nModel comparison plot saved to: {save_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_test, model_name='Random Forest', \n",
    "                             save_path='results/figures/confusion_matrix.png'):\n",
    "        \"\"\"Plot confusion matrix for best model\"\"\"\n",
    "        y_pred = self.results[model_name]['predictions']\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(14, 12))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.label_encoder.classes_,\n",
    "                   yticklabels=self.label_encoder.classes_,\n",
    "                   cbar_kws={'label': 'Count'})\n",
    "        plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Confusion matrix saved to: {save_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_feature_importance(self, feature_names, model_name='Random Forest',\n",
    "                                save_path='results/figures/feature_importance.png'):\n",
    "        \"\"\"Plot feature importance\"\"\"\n",
    "        model = self.models[model_name]\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = model.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(feature_importance_df['feature'], \n",
    "                    feature_importance_df['importance'], \n",
    "                    color='teal')\n",
    "            plt.xlabel('Importance', fontsize=12)\n",
    "            plt.title(f'Feature Importance - {model_name}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "            plt.gca().invert_yaxis()\n",
    "            \n",
    "            # Add percentage labels\n",
    "            for i, v in enumerate(feature_importance_df['importance']):\n",
    "                plt.text(v + 0.005, i, f'{v:.3f} ({v*100:.1f}%)', \n",
    "                        va='center', fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Feature importance plot saved to: {save_path}\")\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\nFeature Importance Ranking:\")\n",
    "            print(feature_importance_df)\n",
    "            \n",
    "            return feature_importance_df\n",
    "        else:\n",
    "            print(f\"{model_name} does not support feature importance\")\n",
    "    \n",
    "    def cross_validate(self, model_name='Random Forest', cv=5):\n",
    "        \"\"\"Perform cross-validation\"\"\"\n",
    "        print()\n",
    "        print(f\"CROSS-VALIDATION ({cv}-Fold) - {model_name}\")\n",
    "        print()\n",
    "        \n",
    "        # Load fresh data\n",
    "        df = pd.read_csv(self.data_path)\n",
    "        X = df.drop('label', axis=1)\n",
    "        y = self.label_encoder.transform(df['label'])\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Perform CV\n",
    "        model = self.models[model_name]\n",
    "        cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        print(f\"Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        return cv_scores\n",
    "    \n",
    "    def save_models(self, best_model_name='Random Forest'):\n",
    "        \"\"\"Save trained models\"\"\"\n",
    "        print(\"SAVING MODELS\")\n",
    "        \n",
    "        # Create models directory\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        \n",
    "        # Save best model\n",
    "        best_model = self.models[best_model_name]\n",
    "        joblib.dump(best_model, f'models/crop_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "        print(f\"Best model saved: models/crop_model_{best_model_name.lower().replace(' ', '_')}.pkl\")\n",
    "        \n",
    "        # Save scaler and encoder\n",
    "        joblib.dump(self.scaler, 'models/scaler.pkl')\n",
    "        joblib.dump(self.label_encoder, 'models/label_encoder.pkl')\n",
    "        print(\"Scaler saved: models/scaler.pkl\")\n",
    "        print(\"Label encoder saved: models/label_encoder.pkl\")\n",
    "        \n",
    "        # Save all models\n",
    "        for model_name, model in self.models.items():\n",
    "            filename = f'models/crop_model_{model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "            joblib.dump(model, filename)\n",
    "        \n",
    "        print(f\"\\n All {len(self.models)} models saved successfully!\")\n",
    "        \n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "\n",
    "    print(\"CROP RECOMMENDATION SYSTEM - MODEL TRAINING\")\n",
    "    \n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = CropRecommendationModel()\n",
    "    \n",
    "    # Train all models\n",
    "    X_test, y_test, feature_names = trainer.train_all_models()\n",
    "    \n",
    "    # Compare models\n",
    "    comparison_df, best_model_name = trainer.compare_models()\n",
    "    \n",
    "    # Plot model comparison\n",
    "    trainer.plot_model_comparison(comparison_df)\n",
    "    \n",
    "    # Plot confusion matrix for best model\n",
    "    trainer.plot_confusion_matrix(y_test, model_name=best_model_name)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    trainer.plot_feature_importance(feature_names, model_name=best_model_name)\n",
    "    \n",
    "    # Cross-validation\n",
    "    trainer.cross_validate(model_name=best_model_name)\n",
    "    \n",
    "    # Save models\n",
    "    trainer.save_models(best_model_name=best_model_name)\n",
    "    \n",
    "    print(\"MODEL TRAINING COMPLETE!\")\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5630611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\Yvonne\\Desktop\\DATA SCIENCE\\FINAL CAPSTONE\\crop-recommendation-system\n",
      "Raw dataset: (8800, 8)\n",
      "Columns: ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall', 'label    ']\n",
      "    N   P   K  temperature   humidity        ph    rainfall label    \n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536      rice\n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537      rice\n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248      rice\n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034      rice\n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340      rice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered dataset: (8800, 31)\n",
      "Columns: ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall', 'label', 'NPK_ratio', 'NP_ratio', 'NK_ratio', 'PK_ratio', 'nutrient_sum', 'nutrient_balance', 'nutrient_dominance', 'temp_humidity_index', 'heat_stress_index', 'water_stress_index', 'moisture_availability', 'evapotranspiration', 'ph_deviation_neutral', 'acidic_soil', 'alkaline_soil', 'climate_zone', 'rainfall_category', 'N_category', 'P_category', 'K_category', 'tropical_suitability', 'temperate_suitability', 'arid_suitability']\n",
      "    N   P   K  temperature   humidity        ph    rainfall label  NPK_ratio  \\\n",
      "0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice   1.046512   \n",
      "1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice   0.850000   \n",
      "2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice   0.600000   \n",
      "3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice   0.973684   \n",
      "4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice   0.917647   \n",
      "\n",
      "   NP_ratio  ...  acidic_soil  alkaline_soil  climate_zone  rainfall_category  \\\n",
      "0  2.093023  ...            0              0     temperate               high   \n",
      "1  1.440678  ...            0              0     temperate               high   \n",
      "2  1.071429  ...            0              1     temperate               high   \n",
      "3  2.055556  ...            0              0     temperate               high   \n",
      "4  1.813953  ...            0              1     temperate               high   \n",
      "\n",
      "   N_category  P_category  K_category  tropical_suitability  \\\n",
      "0        high      medium      medium                     0   \n",
      "1        high      medium      medium                     0   \n",
      "2      medium      medium      medium                     0   \n",
      "3      medium         low         low                     1   \n",
      "4      medium      medium      medium                     0   \n",
      "\n",
      "   temperate_suitability  arid_suitability  \n",
      "0                      0                 0  \n",
      "1                      0                 0  \n",
      "2                      0                 0  \n",
      "3                      0                 0  \n",
      "4                      0                 0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Go up two levels to reach project root\n",
    "os.chdir('../..')\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# Now load the files\n",
    "df1 = pd.read_csv('data/raw/Crop_recommendation.csv')\n",
    "print(f\"Raw dataset: {df1.shape}\")\n",
    "print(f\"Columns: {df1.columns.tolist()}\")\n",
    "print(df1.head())\n",
    "\n",
    "df2 = pd.read_csv('data/processed/crop_data_with_features.csv')\n",
    "print(f\"Engineered dataset: {df2.shape}\")\n",
    "print(f\"Columns: {df2.columns.tolist()}\")\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6490ddaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
